{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "stem_url = \"https://raw.githubusercontent.com/James-G-Smith/Python-projects/master/Baker%20Hughes%20Activity%20duration/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Power of Predictive Analytics \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Hack 6 Baker Hughes asked the community to find out what caused activity duration variance and how to predict it using machine learning. During the Hack Team 9B developed a solution, which was further refined by the Projecting Success. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set includes the following features:\n",
    "- ID\n",
    "- Project Number\n",
    "- Product Line Number\n",
    "- Department Number\n",
    "- Activity Type Number\n",
    "- Code*\n",
    "- Class Number\n",
    "- Baseline Start Date\n",
    "- Baseline Finish Date\n",
    "- Planned Duration\n",
    "- Forecast Start Date\n",
    "- Forecast Finish Date\n",
    "- Forecast Duration\n",
    "- Project Duration\n",
    "\n",
    "*Note that the code feature is hierarchical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Import libraries\n",
    "\n",
    "We import multiple libraries. All are listed below with there uses.\n",
    "\n",
    "- Pandas - Library for the manipulation of panel data. Think excel.\n",
    "- Numpy - Library for the manipulation of numerical data.\n",
    "- Matplotlib - Library for making plots.\n",
    "- Scikit Learn - Library for Machine Learning (This will be imported later).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## IMPORT PACKAGES ##########\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Import Data\n",
    "\n",
    "The data is stored on two excel sheets. These two are imported and merged.\n",
    "\n",
    "Also, the data is shuffled this is to ensure the data is well mixed. This is needed because the data is in order of activities and if we do not shuffle it there will be unnecessary bias in the data, making the final model worse.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "schedule_data = pd.read_csv(stem_url + r\"data/P6_Schedule_Data.csv\")\n",
    "project_data = pd.read_csv(stem_url + r\"data/Project_Duration.csv\")\n",
    "\n",
    "# merge data\n",
    "data = pd.merge(schedule_data, project_data, on='ProjectNumber')\n",
    "\n",
    "# shuffle data\n",
    "data = data.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Change Data Type and Clean Data\n",
    "\n",
    "The data in it's raw form can not be processed by the machine learning model. It needs to be cleaned and converted to the correct data type.\n",
    "\n",
    "### Converting Data types\n",
    "\n",
    "In python data can be stored in different types such as string(text data), integers(numerical data), datetime(Date Time Data) and many other data types.\n",
    "\n",
    "Data features below are converted to a datetime data type. To allow for the model to proccess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data columns to datetime\n",
    "data['Baseline Start Date'] = pd.to_datetime(data['Baseline Start Date'])\n",
    "data['Baseline Finish Date'] = pd.to_datetime(data['Baseline Finish Date'])\n",
    "data['Forecast Start Date'] = pd.to_datetime(data['Forecast Start Date'])\n",
    "data['Forecast Finish Date'] = pd.to_datetime(data['Forecast Finish Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often data can be divided into different into groups. In this case we can divide data on the department number into different groups, as the department number can be 1,2,3,4,5,6,7 or 8. When data can be divided like this we call this **categorical data**.    \n",
    "\n",
    "There many more examples of categorical data in our data such as:\n",
    "\n",
    "- ActivityTypeNumber\n",
    "- ProjectNumber\n",
    "- ProductLineNumber\n",
    "- ActivityTypeNumber\n",
    "- ClassNumber\n",
    "\n",
    "All our categorical data is current saved as **integers**. This will confuse the model later down the line, so it needs to be converted to a **string**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['ActivityTypeNumber'].dtypes)\n",
    "print(data['ProjectNumber'].dtypes)\n",
    "print(data['ProductLineNumber'].dtypes)\n",
    "print(data['DepartmentNumber'].dtypes)\n",
    "print(data['ActivityTypeNumber'].dtypes)\n",
    "print(data['ClassNumber'].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical data needs to be converted to **strings**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert other columns to strings\n",
    "data['ActivityTypeNumber'] = data['ActivityTypeNumber'].apply(str)\n",
    "data['ProjectNumber'] = data['ProjectNumber'].apply(str)\n",
    "data['ProductLineNumber'] = data['ProductLineNumber'].apply(str)\n",
    "data['DepartmentNumber'] = data['DepartmentNumber'].apply(str)\n",
    "data['ActivityTypeNumber'] = data['ActivityTypeNumber'].apply(str)\n",
    "data['ClassNumber'] = data['ClassNumber'].apply(str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning\n",
    "\n",
    "Currently, the data includes data on **complete** and **incomplete activities**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['ACTIVITY_STATUS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to look at **completed** activities only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_data = data[data['ACTIVITY_STATUS'] == 'Completed']\n",
    "print(completed_data['ACTIVITY_STATUS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The activity status needs to be removed, as it provides little insight.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_data = completed_data.drop(columns = ['ACTIVITY_STATUS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.Feature Creation \n",
    "\n",
    "One of the best ways to improve the quality of our model is to create **new features** from the **current features**.\n",
    "\n",
    "### Months \n",
    "\n",
    "Currently, the datetime, which includes day, month and year, is saved as one feature. This can be broken into day, month and year. \n",
    "\n",
    "To explain how to do this below we extract the month from the start date features and then save them as a new feature.\n",
    "\n",
    "The day and the year will be extracted in an exercise at the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create baseline & forecast start months\n",
    "completed_data['Baseline Start Month'] = completed_data['Baseline Start Date'].dt.month\n",
    "completed_data['Forecast Start Month'] = completed_data['Forecast Start Date'].dt.month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code\n",
    "\n",
    "Currently, the code feature is **hierarchical**. This means that data set has a tree structure, where by elements are connect to parent element. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"img/Hierachial%20Structure.PNG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(stem_url + filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see below the hierachical structure has not been broken down. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(completed_data['Code'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script below splits the code column into each of it's constitute codes. Then the codes are reassambled into new columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split code column.\n",
    "def SplitDots(cell):\n",
    "    return cell.split('.')\n",
    "\n",
    "completed_data[\"CodeSplit\"] = completed_data[\"Code\"].apply(SplitDots) \n",
    "\n",
    "# Now we assemble all the codes.\n",
    "\n",
    "def Assemble(cell,level):\n",
    "    elements = len(cell)\n",
    "    if elements < level:\n",
    "        return \"\"\n",
    "    else:\n",
    "        code = str(cell[0])\n",
    "        if level > 1:\n",
    "            for i in range(level-1):\n",
    "                code = code + \".\" + str(cell[i+1])\n",
    "            return(code)\n",
    "        else:\n",
    "            return(code)\n",
    "\n",
    "completed_data[\"Code 1\"] = completed_data[\"CodeSplit\"].apply(Assemble,level=1)     \n",
    "completed_data[\"Code 2\"] = completed_data[\"CodeSplit\"].apply(Assemble,level=2)    \n",
    "completed_data[\"Code 3\"] = completed_data[\"CodeSplit\"].apply(Assemble,level=3)    \n",
    "completed_data[\"Code 4\"] = completed_data[\"CodeSplit\"].apply(Assemble,level=4)    \n",
    "completed_data[\"Code 5\"] = completed_data[\"CodeSplit\"].apply(Assemble,level=5)    \n",
    "completed_data[\"Code 6\"] = completed_data[\"CodeSplit\"].apply(Assemble,level=6)    \n",
    "completed_data[\"Code 7\"] = completed_data[\"CodeSplit\"].apply(Assemble,level=7)    \n",
    "completed_data[\"Code 8\"] = completed_data[\"CodeSplit\"].apply(Assemble,level=8)    \n",
    "completed_data[\"Code 9\"] = completed_data[\"CodeSplit\"].apply(Assemble,level=9)    \n",
    "completed_data[\"Code 10\"] = completed_data[\"CodeSplit\"].apply(Assemble,level=10)\n",
    "\n",
    "# remove original code column\n",
    "completed_data = completed_data.drop(columns = ['Code'])\n",
    "completed_data = completed_data.drop(columns = ['CodeSplit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(completed_data['Code 1'])\n",
    "print('')\n",
    "print(completed_data['Code 2'])\n",
    "print('')\n",
    "print(completed_data['Code 3'])\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Variables \n",
    "\n",
    "Our data contains a lot of categorical data. Unfortunately, this data can not be processed by our model algorithm. Therefore, we convert our data to dummy variables. A dummy variable is a series of 0s and 1s that can represent a categorgy, which can be processed by the modelling algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy variables for categorical data\n",
    "project_dummies = pd.get_dummies(completed_data['ProjectNumber'], prefix='Project ')\n",
    "product_dummies = pd.get_dummies(completed_data['ProductLineNumber'], prefix='Product Line ')\n",
    "department_dummies = pd.get_dummies(completed_data['DepartmentNumber'], prefix='Department ')\n",
    "activity_dummies = pd.get_dummies(completed_data['ActivityTypeNumber'], prefix='Activity Type ')\n",
    "start_month_dummies = pd.get_dummies(completed_data['Baseline Start Month'], prefix='Month ')\n",
    "code_1 =  pd.get_dummies(completed_data['Code 1'], prefix='Code 1')\n",
    "code_2 =  pd.get_dummies(completed_data['Code 2'], prefix='Code 2')\n",
    "code_3 =  pd.get_dummies(completed_data['Code 3'], prefix='Code 3')\n",
    "code_4 =  pd.get_dummies(completed_data['Code 4'], prefix='Code 4')\n",
    "code_5 =  pd.get_dummies(completed_data['Code 5'], prefix='Code 5')\n",
    "code_6 =  pd.get_dummies(completed_data['Code 6'], prefix='Code 6')\n",
    "code_7 =  pd.get_dummies(completed_data['Code 7'], prefix='Code 7')\n",
    "code_8 =  pd.get_dummies(completed_data['Code 8'], prefix='Code 8')\n",
    "code_9 =  pd.get_dummies(completed_data['Code 9'], prefix='Code 9')\n",
    "class_dummies = pd.get_dummies(completed_data['ClassNumber'], prefix='Class ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate Data\n",
    "\n",
    "All the new features need to be concatenated (merged)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate final data\n",
    "final_data = pd.concat([project_dummies, product_dummies, department_dummies, activity_dummies, start_month_dummies, \n",
    "                        code_1, code_2, code_3, code_4, code_5, code_6, code_7, code_8, code_9, class_dummies, \n",
    "                        completed_data[['Planned Duration','Forecast Duration','ProjectDuration']]], axis=1, sort=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.Prepare Data\n",
    "\n",
    "The data needs to be split into features (X_data) and target(what we are predicting(y_data)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into features & target\n",
    "X_data = final_data.drop(columns = ['Forecast Duration'])\n",
    "y_data = final_data['Forecast Duration']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalising the Data\n",
    "\n",
    "Each feature has a **different scale**. This will be a problem as some features will have a greater influence on the model output than others. Therefore, they need to be placed on a common scale, which can be achieved by normalising the data. The StandardScaler function does this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# scale the data\n",
    "scalerX = StandardScaler().fit(X_data)\n",
    "scalery = StandardScaler().fit(y_data.values.reshape(-1, 1))\n",
    "X_data = scalerX.transform(X_data)  \n",
    "y_data = scalery.transform(y_data.values.reshape(-1, 1))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Data\n",
    "\n",
    "Before the data can be put into the model it needs to be split into a training and a test data set. \n",
    "\n",
    "In splitting the data we create a training set, which is used to create the model and a test set that is used to evaluate the model. \n",
    "\n",
    "To split the data the train_test_split function from the scikit-learn library is used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split training & validation data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data,  test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Modelling\n",
    "\n",
    "With the data split, it is now possible to build the model. In this case a **gradient boost regressor** model from scikit-learn is used. \n",
    "\n",
    "Inside the **GradientBoostingRegressor function** it is possible to set **hyperparameters**, which are parameters set before the model is created that control the **learning process**. \n",
    "\n",
    "Once the hyperparameters have been set, the model can be **fitted** with the **training data**, as seen with **gradient_boosting_model.fit()** function.\n",
    "\n",
    "Once the **training data** has been fitted to the model, it is time to make a **prediction** based off the **test data**. This is done with **gradient_boosting_model.predict()** function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\james\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1454: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Creation of the model.\n",
    "gradient_boosting_model = GradientBoostingRegressor(n_estimators=50, max_depth=10)\n",
    "# Fitting training data on the model.\n",
    "gradient_boosting_model = gradient_boosting_model.fit(X_train, y_train)\n",
    "# Predict based off the X_Test\n",
    "gradient_boosting_predictions = gradient_boosting_model.predict(X_test)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.Model Evaluation\n",
    "\n",
    "To evaluate the model R^2 is used.\n",
    "\n",
    "### **R^2** - Coefficient of Determination\n",
    "\n",
    "It is a measure how much the model's output variance is influenced by the input variance. \n",
    "\n",
    "i.e.  If the R^2 of a model is 0.50, then approximately half of the observed variation can be explained by the model's inputs.\n",
    "\n",
    "The closer to 0.0 the worse the fit and the closer to 1.0 the better the fit.\n",
    "\n",
    "The **r2_score** function can be used to calculate **R^2**.\n",
    "\n",
    "It's important to note scalery.inverse_transform was used to reverse the normalisation implemented before.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2_score = r2_score(scalery.inverse_transform(y_test), scalery.inverse_transform(gradient_boosting_predictions))\n",
    "\n",
    "print('Validation R2 Score: ', r2_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "\n",
    "To understand the influence of each **feature** on the prediction, we calculate the **feature importance**. Feature importance refers to techniques that assign a score to input features based on how useful they are at predicting a target variable.\n",
    "\n",
    "In this case we only plot the five most infuential features, as the rest have a negligible influence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get feature importance\n",
    "features = final_data.drop(columns = ['Forecast Duration']).columns.values\n",
    "importance  = gradient_boosting_model.feature_importances_ \n",
    "features_df = pd.DataFrame({'Feature':features, 'Importance':importance})\n",
    "\n",
    "# plot feature importance\n",
    "top10_features = features_df.nlargest(5,'Importance')\n",
    "top10_features.plot.bar(x='Feature')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Add up all the values in the planned duration column in the **completed data** set.\n",
    "\n",
    "The completed_data is a pandas data frame so you'll need to use this code -> df.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a new feature which only includes the date  from the Baseline Start Date from the **completed data** set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create a new feature which only includes the year from the Baseline Start Date from the **completed data** set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  Convert the Project Number from the completed data set feature into an integer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
